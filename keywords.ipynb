{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## datos - corpus introducciones revista CELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "from nltk import sent_tokenize\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import random\n",
    "import operator\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('corpus_cell.json','r')\n",
    "data_cell=data.read()\n",
    "data_cell = ast.literal_eval(data_cell)\n",
    "introductions=[' '.join(item['introduction']) for item in data_cell]\n",
    "introductions=[re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", item) for item in introductions]\n",
    "introductions=[x for x in introductions if len(x)>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=open('stopwords.txt','r')\n",
    "data_read = data.read()\n",
    "stop_words=data_read.replace('\\n',' ').split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    sentences=sent_tokenize(text)\n",
    "    sentences=[nlp(sentence) for sentence in sentences]\n",
    "    ## nos quedamos, en esta version, solo con sustantivos (propios y comunes) y adjetivos!\n",
    "    sentences=[[token.lemma_ for token in sentence if token.pos_=='NOUN' or token.pos_=='ADJ' or token.pos_=='PROPN'] for sentence in sentences]\n",
    "    text=[item for sublist in sentences for item in sublist]\n",
    "    text=[word for word in text if not word in stop_words]\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The emerging outbreak of Zika virus in the Americas has brought this once obscure pathogen to the forefront of global healthcare. Mostly transmitted by Aedes aegypti and A.\\xa0albopictus mosquitoes, Zika virus infections have been further spread by international travel and have expanded to large, heavily populated regions of South, Central, and North America . Correlations between the increase in Zika virus infections, the development of fetal microcephaly , and Guillain-Barr√© syndrome have resulted in the declaration of a public health emergency by the World Health Organization  and a call for fast-tracked development of Zika virus diagnostics . Synthetic biology is an emerging discipline that has great potential to respond to such pandemics. The increasing ability of synthetic biologists to repurpose and engineer natural biological components for practical applications has led to new opportunities for molecular diagnostics . We previously developed two biotechnologies that dramatically lower the cost of and technical\\xa0barriers to the development of synthetic biology-based diagnostics. The first technology, programmable RNA sensors called toehold switches, can be rationally designed to bind and sense virtually any RNA sequence . The second technology, a freeze-dried, paper-based, cell-free protein expression platform, allows for the deployment of these toehold switch sensors outside of a research laboratory by providing a sterile and abiotic method for the storage and distribution of genetic circuits at room temperature . We combined these technologies to create a platform for rapidly and inexpensively developing and deploying diagnostic sensors. In the context of the Zika virus outbreak, the paper-based sensors offer a solution to the critical challenges facing diagnosis of the virus. Standard serological approaches, such as antibody detection, are limited in diagnostic value due to cross-reactivity in patients that have previously been infected by other flaviviruses circulating in the region. As a result, accurate diagnosis requires nucleic acid-based detection methods, such as PCR and isothermal nucleic acid amplification . However, such techniques are relatively expensive, require technical expertise to run and interpret, and utilize equipment that is incompatible with use in remote and low-resource locations where surveillance and containment are critically needed. Here, we demonstrate the rapid development of a diagnostic workflow for sequence-specific detection of Zika virus that can be employed in low-resource settings . We have addressed limitations in the practical deployment of nucleic acid-based molecular diagnostics by combining isothermal RNA amplification with toehold switch sensors on our freeze-dried, paper-based platform. We automate the amplification primer and sensor design process using in\\xa0silico algorithms and demonstrate a high-throughput pipeline to assemble and test 48 Zika sensors in less than 7\\xa0hr. Clinically relevant sensitivity is attained using our amplification and detection scheme, and we report no significant detection of the closely related Dengue virus. To further increase diagnostic capabilities, we develop a CRISPR/Cas9-based module that discriminates between Zika genotypes with single-base resolution. Finally, we employ a simple sample-preparation protocol to reliably extract viral RNA and demonstrate robust detection with this scheme using active Zika virus samples.'"
      ]
     },
     "execution_count": 640,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=introductions[0]\n",
    "text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grafos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_weighted(text,K):\n",
    "    text=clean(text)\n",
    "    unique_words=list(set(text))\n",
    "    G=nx.Graph()\n",
    "    for word in unique_words:\n",
    "        G.add_node(word)\n",
    "    for word in text: ## recorremos el texto y encontramos los indices de todas las aparicions de word (index_word)\n",
    "        index_word=[index for index, value in enumerate(text) if value == word]\n",
    "        ## ahora buscamos las palabras vecinas en una ventana de largo K (hacia adelante)\n",
    "        for index in index_word:\n",
    "            for k in range(1,K+1):\n",
    "                if index+k in range(len(text)):\n",
    "                    if G.has_edge(text[index],text[index+k])==False:\n",
    "                        G.add_edge(text[index],text[index+k],weight=1)\n",
    "                    else:\n",
    "                        x=G[text[index]][text[index+k]]['weight']\n",
    "                        G[text[index]][text[index+k]]['weight']=x+1\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extraccion de keywords!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjacency(list_keywords,text):\n",
    "    text=clean(text)\n",
    "    new_list=list_keywords[:]\n",
    "    for pair in list(itertools.permutations(list_keywords,2)):\n",
    "        index_0=text.index(pair[0])\n",
    "        index_1=text.index(pair[1])\n",
    "        if index_0==index_1-1:\n",
    "            if pair[0] in new_list:\n",
    "                new_list.remove(pair[0])\n",
    "            if pair[1] in new_list:\n",
    "                new_list.remove(pair[1])\n",
    "            if pair[0]+' '+pair[1] not in new_list:\n",
    "                new_list+=[pair[0]+' '+pair[1]]\n",
    "        if index_0==index_1+1:\n",
    "            if pair[0] in new_list:\n",
    "                new_list.remove(pair[0])\n",
    "            if pair[1] in new_list:\n",
    "                new_list.remove(pair[1])\n",
    "            if pair[1]+' '+pair[0] not in new_list:\n",
    "                new_list+=[pair[1]+' '+pair[0]]\n",
    "           \n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('virus', 'zika', 'diagnostic', 'sensor', 'detection')"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def keywords_pagerank(text,number,adjacency_bool):\n",
    "    G=graph_weighted(text,4)\n",
    "    keywords=nx.pagerank(G, alpha=0.85, weight='weight')\n",
    "    if adjacency_bool==True:\n",
    "        return adjacency(list(list(zip(*sorted(keywords.items(), key=operator.itemgetter(1),reverse=True)))[0][:number]),text)\n",
    "    else:\n",
    "        return list(zip(*sorted(keywords.items(), key=operator.itemgetter(1),reverse=True)))[0][:number]\n",
    "\n",
    "        \n",
    "keywords_pagerank(text,5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### betweenness centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keywords_betweenness_centrality(text,number,adjacency_bool):\n",
    "    G=graph_weighted(text,4)\n",
    "    keywords=nx.betweenness_centrality(G)\n",
    "    if adjacency_bool==True:\n",
    "        return adjacency(list(list(zip(*sorted(keywords.items(), key=operator.itemgetter(1),reverse=True)))[0][:number]),text)\n",
    "    else:\n",
    "        return list(zip(*sorted(keywords.items(), key=operator.itemgetter(1),reverse=True)))[0][:number]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['diagnostic', 'sensor', 'detection', 'zika virus']"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_betweenness_centrality(text,5,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d6543481b214a4ab57642c6aba45e10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.visualization>"
      ]
     },
     "execution_count": 642,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def visualization(text,K,number_keywords):\n",
    "    G=graph_weighted(text,K)\n",
    "    keywords=nx.pagerank(G, alpha=0.85, weight='weight')\n",
    "    list_keywords=keywords_pagerank(text,number_keywords,False)\n",
    "    G = G.subgraph(list_keywords)\n",
    "\n",
    "    pos = nx.spring_layout(G)\n",
    "    labels={i:i for i in G.nodes}\n",
    "    nx.draw_networkx_nodes(G, pos, G.nodes, node_size = [keywords[k] * 10000 for k in G.nodes])\n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.5,width=0.5)\n",
    "    nx.draw_networkx_labels(G,pos,labels,font_size=12)\n",
    "    plt.axis('off')\n",
    "    #plt.savefig('keywords.eps', format='eps', transparent=True, bbox_inches='tight',dpi=800)\n",
    "    plt.show()\n",
    "\n",
    "from ipywidgets import *\n",
    "style = {'description_width': 'initial'}\n",
    "\n",
    "interact(visualization,text=text,K=widgets.IntSlider(min=1,max=20,step=1,value=4,description='window size', style=style),number_keywords=widgets.IntSlider(min=1,max=30,step=1,value=15,description='number of keywords', style=style))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sentence detection!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_verb(text):\n",
    "    sentences=sent_tokenize(text)\n",
    "    sentences=[nlp(sentence) for sentence in sentences]\n",
    "    ## nos quedamos, en esta version, solo con sustantivos (propios y comunes) y adjetivos!\n",
    "    sentences=[[token for token in sentence if token.lemma_!='-PRON-'] for sentence in sentences]\n",
    "    sentences=[[token.lemma_ for token in sentence if token.pos_=='VERB' or token.pos_=='NOUN' or token.pos_=='ADJ' or token.pos_=='PROPN'] for sentence in sentences]\n",
    "    text=[item for sublist in sentences for item in sublist]\n",
    "    #text=[word for word in text if not word in stop_words]\n",
    "    return text\n",
    "\n",
    "def graph_weighted_detection(text,K):\n",
    "    text=clean_verb(text)\n",
    "    unique_words=list(set(text))\n",
    "    G=nx.Graph()\n",
    "    for word in unique_words:\n",
    "        G.add_node(word)\n",
    "    for word in text: ## recorremos el texto y encontramos los indices de todas las aparicions de word (index_word)\n",
    "        index_word=[index for index, value in enumerate(text) if value == word]\n",
    "        ## ahora buscamos las palabras vecinas en una ventana de largo K (hacia adelante)\n",
    "        for index in index_word:\n",
    "            for k in range(1,K+1):\n",
    "                if index+k in range(len(text)):\n",
    "                    if G.has_edge(text[index],text[index+k])==False and G.has_edge(text[index+k],text[index])==False:\n",
    "                        G.add_edge(text[index],text[index+k],weight=1)\n",
    "                    else:\n",
    "                        x=G[text[index]][text[index+k]]['weight']\n",
    "                        G[text[index]][text[index+k]]['weight']=x+1\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto='In the language as recorded in a modern English dictionary the great majority of words are borrowed; but the words we ordinarily use in speaking are largely of English origin, although for the most part somewhat changed in form since their first introduction into England.'\n",
    "#texto='Although (as has been shown above) it would be incorrect to say that English was derived from Latin, or French, or Greek, of from anything else but the original language of the Teutonic branch of the Indo-European language, nevertheless Latin, French and Greek have not been without great and lasting influence on our vocabulary.'\n",
    "#texto='The majority of words recorded in a modern English dictionary have been borrowed from other languages.'\n",
    "#texto='The majority of words recorded in a modern English dictionary have been borrowed from other languages. However, the words ordinarily used in speaking are largely of English origin. Most words have somewhat changed in form since their first introduction into England.'\n",
    "#texto='The emerging outbreak of Zika virus in the Americas has brought this once obscure pathogen to the forefront of global healthcare . Mostly transmitted by Aedes aegypti and A.   albopictus mosquitoes , Zika virus infections have been further spread by international travel and have expanded to large , heavily populated regions of South , Central , and North America . '\n",
    "G=graph_weighted_detection(texto,10)\n",
    "import community\n",
    "partition=community.best_partition(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37mIn\u001b[0m \u001b[1m\u001b[37mthe\u001b[0m \u001b[1m\u001b[34mlanguage\u001b[0m \u001b[1m\u001b[37mas\u001b[0m \u001b[1m\u001b[34mrecorded\u001b[0m \u001b[1m\u001b[37min\u001b[0m \u001b[1m\u001b[37ma\u001b[0m \u001b[1m\u001b[34mmodern\u001b[0m \u001b[1m\u001b[34mEnglish\u001b[0m \u001b[1m\u001b[34mdictionary\u001b[0m \u001b[1m\u001b[37mthe\u001b[0m \u001b[1m\u001b[34mgreat\u001b[0m \u001b[1m\u001b[34mmajority\u001b[0m \u001b[1m\u001b[37mof\u001b[0m \u001b[1m\u001b[34mwords\u001b[0m \u001b[1m\u001b[31mare\u001b[0m \u001b[1m\u001b[34mborrowed\u001b[0m \u001b[1m\u001b[37m;\u001b[0m \u001b[1m\u001b[37mbut\u001b[0m \u001b[1m\u001b[37mthe\u001b[0m \u001b[1m\u001b[34mwords\u001b[0m \u001b[1m\u001b[37mwe\u001b[0m \u001b[1m\u001b[37mordinarily\u001b[0m \u001b[1m\u001b[34muse\u001b[0m \u001b[1m\u001b[37min\u001b[0m \u001b[1m\u001b[31mspeaking\u001b[0m \u001b[1m\u001b[31mare\u001b[0m \u001b[1m\u001b[37mlargely\u001b[0m \u001b[1m\u001b[37mof\u001b[0m \u001b[1m\u001b[34mEnglish\u001b[0m \u001b[1m\u001b[31morigin\u001b[0m \u001b[1m\u001b[37m,\u001b[0m \u001b[1m\u001b[37malthough\u001b[0m \u001b[1m\u001b[37mfor\u001b[0m \u001b[1m\u001b[37mthe\u001b[0m \u001b[1m\u001b[31mmost\u001b[0m \u001b[1m\u001b[31mpart\u001b[0m \u001b[1m\u001b[37msomewhat\u001b[0m \u001b[1m\u001b[31mchanged\u001b[0m \u001b[1m\u001b[37min\u001b[0m \u001b[1m\u001b[31mform\u001b[0m \u001b[1m\u001b[37msince\u001b[0m \u001b[1m\u001b[37mtheir\u001b[0m \u001b[1m\u001b[31mfirst\u001b[0m \u001b[1m\u001b[31mintroduction\u001b[0m \u001b[1m\u001b[37minto\u001b[0m \u001b[1m\u001b[31mEngland\u001b[0m \u001b[1m\u001b[37m.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from termcolor import colored\n",
    "\n",
    "colors = ['blue', 'red', 'green', 'magenta','grey','cyan','yellow','white']\n",
    "C=[]\n",
    "#texto=clean_verb(texto)\n",
    "texto_nlp=nlp(texto)\n",
    "for token in texto_nlp:\n",
    "    if token.lemma_ in partition.keys():\n",
    "        C+=[colored(token.text, colors[partition[token.lemma_]],attrs=['bold'])]\n",
    "    else:\n",
    "        C+=[colored(token.text, colors[7],attrs=['bold'])]\n",
    "\n",
    "#print(' '.join(C))\n",
    "\n",
    "def communities(texto,K):\n",
    "    G=graph_weighted_detection(texto,K)\n",
    "    \n",
    "    partition=community.best_partition(G)\n",
    "    C=[]\n",
    "    texto_nlp=nlp(texto)\n",
    "    texto=clean_verb(texto)\n",
    "    ###############################\n",
    "    #for i in range(len(texto_nlp)):\n",
    "    #    if texto_nlp[i].lemma_ in partition.keys() and texto_nlp[i-1].lemma_ in partition.keys() and texto_nlp[i+1].lemma_ in partition.keys():\n",
    "    #        if partition[texto_nlp[i-1].lemma_]==partition[texto_nlp[i+1].lemma_] and partition[texto_nlp[i].lemma_]!=partition[texto_nlp[i-1].lemma_]:\n",
    "    #            partition[texto_nlp[i].lemma_]=partition[texto_nlp[i+1].lemma_]\n",
    "    ###############################\n",
    "    #text_clean=clean_verb(texto)\n",
    "    for i in range(len(texto_nlp)):\n",
    "        if texto_nlp[i].lemma_ in partition.keys():\n",
    "            C+=[colored(texto_nlp[i].text, colors[partition[texto_nlp[i].lemma_]],attrs=['bold'])]\n",
    "        else:\n",
    "            C+=[colored(texto_nlp[i].text, colors[7],attrs=['bold'])]\n",
    "\n",
    "    print(' '.join(C))\n",
    "\n",
    "communities(texto,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

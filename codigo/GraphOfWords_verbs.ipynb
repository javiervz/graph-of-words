{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keywords\n",
    "## En este notebook se modifica la funcion clean (que aparece en GraphOfWords.ipynb) para quedarnos unicamente con verbos. Las demas funciones se modifican para estar de acuerdo con esto.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import spacy\n",
    "import operator\n",
    "nlp = spacy.load('en')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import sent_tokenize\n",
    "import re\n",
    "import string\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## funcion que limpia los textos. Ahora nos quedamos solo con los verbos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_verbs(text):\n",
    "    text=re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", text)\n",
    "    sentences=sent_tokenize(text)\n",
    "    sentences=[nlp(sentence) for sentence in sentences]\n",
    "    sentences=[[token.lemma_ for token in sentence if token.pos_=='VERB'] for sentence in sentences]\n",
    "    text=[item for sublist in sentences for item in sublist]\n",
    "    text=[word for word in text if not word in stop_words]\n",
    "    table=str.maketrans('', '', string.punctuation)\n",
    "    text = [w.translate(table) for w in text] ## removemos los simbolos que funcionan como sustantivos, p.e., %\n",
    "    text = list(filter(lambda a: a != '', text))\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sequence',\n",
       " 'confirm',\n",
       " 'leave',\n",
       " 'inherit',\n",
       " 'inherit',\n",
       " 'indicate',\n",
       " 'occur',\n",
       " 'understand',\n",
       " 'identify',\n",
       " 'inherit',\n",
       " 'identify',\n",
       " 'include',\n",
       " 'incorporate',\n",
       " 'utilize',\n",
       " 'identify',\n",
       " 'archaic',\n",
       " 'compare',\n",
       " 'include',\n",
       " 'search',\n",
       " 'find',\n",
       " 'leverage',\n",
       " 'carry',\n",
       " 'base',\n",
       " 'expect',\n",
       " 'maintain',\n",
       " 'result',\n",
       " 'develop',\n",
       " 'increase',\n",
       " 'apply',\n",
       " 'sequence',\n",
       " 'identify',\n",
       " 'examine',\n",
       " 'match',\n",
       " 'sequence',\n",
       " 'contain',\n",
       " 'obtain']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text='Sequencing the Neanderthal genome (Green et al., 2010, Prüfer et al., 2014), the Denisovan genome (Reich et al., 2010), and several early modern human genomes from Eurasia (Fu et al., 2014, Fu et al., 2015) has confirmed that archaic hominins left their mark in the genomes of modern humans (Plagnol and Wall, 2006, Sankararaman et al., 2014, Vernot and Akey, 2014, Vernot et al., 2016). Present-day individuals in Eurasia inherited ∼2% of their genome from Neanderthals (Green et al., 2010), and individuals from Oceania inherited ∼5% of their genome from Denisovans (Reich et al., 2010). Suggestive evidence indicates that admixture from other unidentified hominin species occurred in Africa (Hammer et al., 2011, Hsieh et al., 2016, Lachance et al., 2012, Plagnol and Wall, 2006, Wall et al., 2009). To understand the functional, phenotypic, and evolutionary consequences of archaic admixture, it is necessary to identify the specific haplotypes and alleles that were inherited from archaic hominin ancestors (Huerta-Sánchez et al., 2014, Juric et al., 2016, Sankararaman et al., 2014, Simonti et al., 2016, Vernot and Akey, 2014). Approaches to identifying introgressed haplotypes include methods that specifically incorporate reference archaic hominin genome sequences and reference-free methods that do not utilize such information. An example of the former category is the method of Sankararaman et al. (2014), which identifies archaic haplotypes by comparing modern human haplotypes to a reference archaic sequence. The latter category of methods include the S∗ statistic (Plagnol and Wall, 2006), which searches for the mutational signature that ancient admixture leaves in the genomes of present-day humans. The S∗ approach is powerful for finding introgressed haplotypes in the absence of an archaic reference genome because it leverages the unusual mutational characteristics of introgressed haplotypes. Because of the long divergence time between Neanderthals and modern humans, Neanderthals carry many alleles that are specific to their lineage. Such alleles are present on introgressed haplotypes but are absent or rare in African genomes. Further, based on the recent timing of admixture, introgressed haplotypes are expected to be maintained without recombination over distances of approximately 50 kb on average (Sankararaman et al., 2012), resulting in high levels of linkage disequilibrium (LD) between Neanderthal-specific alleles in non-African human genomes. In this study, we develop an S∗-like method that has increased power and is suitable for large-scale genome-wide data. We apply the method to large sets of sequenced data from Eurasia and Oceania and identify putative archaic-specific alleles. We examine the rate at which these alleles match the sequenced archaic genomes and the role of the genes containing these alleles, to obtain insights into the history of the admixture events and their impact on modern human genomes.'\n",
    "clean_verbs(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grafo de palabras modificado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K es el largo de la ventana\n",
    "# digraph indica el tipo de grafo- True = dirigido, False = no dirigido\n",
    "def graph_weighted_verbs(text,K,digraph):\n",
    "    text=clean_verbs(text)\n",
    "    unique_words=list(set(text))\n",
    "    if digraph==True: ## grafo dirigido o no dirigido\n",
    "        G=nx.DiGraph()\n",
    "    else:\n",
    "        G=nx.Graph()\n",
    "    for word in unique_words:\n",
    "        G.add_node(word)\n",
    "    for word in unique_words: ## recorremos el texto y encontramos los indices de todas las aparicions de word (index_word)\n",
    "        index_word=[index for index, value in enumerate(text) if value == word]\n",
    "        ## ahora buscamos las palabras vecinas en una ventana de largo K (hacia adelante)\n",
    "        for index in index_word:\n",
    "            for k in range(1,K+1):\n",
    "                if index+k in range(len(text)):\n",
    "                    if G.has_edge(text[index],text[index+k])==False:\n",
    "                        G.add_edge(text[index],text[index+k],weight=1)\n",
    "                    else:\n",
    "                        x=G[text[index]][text[index+k]]['weight']\n",
    "                        G[text[index]][text[index+k]]['weight']=x+1\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<networkx.classes.digraph.DiGraph at 0x7f35ad7d0128>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_weighted_verbs(text,4,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keywords segun pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K es el largo de la ventana\n",
    "# number_keywords indica el numero de keywords\n",
    "# digraph indica si queremos un grafo dirigido (True) o no dirigido (False)\n",
    "def keywords_pagerank_verbs(text,number_keywords,K,digraph):\n",
    "    n=len(clean_verbs(text))\n",
    "    G=graph_weighted_verbs(text,K,digraph)\n",
    "    keywords=nx.pagerank(G, alpha=0.85, weight='weight')\n",
    "    if n<number_keywords: ## en el caso de que el texto sea muy corto (incluso menor al numero de keywords)\n",
    "        number_keywords=n\n",
    "    return list(list(zip(*sorted(keywords.items(), key=operator.itemgetter(1),reverse=True)))[0][:number_keywords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['identify',\n",
       " 'sequence',\n",
       " 'obtain',\n",
       " 'inherit',\n",
       " 'include',\n",
       " 'apply',\n",
       " 'increase',\n",
       " 'develop',\n",
       " 'result',\n",
       " 'maintain']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_pagerank_verbs(text,10,4,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) keywords segun main core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K es el largo de la ventana\n",
    "# digraph es el tipo de grafo - True = grafo dirigido, False = grafo no dirigido\n",
    "# en esta funcion, no es necesario indicar el numero de keywords. Fijamos un maximo de 10 (esto puede ser un parametro)\n",
    "def keywords_kcore_verbs(text,K,digraph):\n",
    "    G=graph_weighted_verbs(text,K,digraph)\n",
    "    G.remove_edges_from(nx.selfloop_edges(G)) ## evitamos ciclos para evitar que Networkx entregue un error\n",
    "    main_core_nodes=list(nx.k_core(G).nodes())\n",
    "    n=len(main_core_nodes)\n",
    "    if n>10:\n",
    "        return main_core_nodes[:10]\n",
    "    else:\n",
    "        return main_core_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['incorporate',\n",
       " 'sequence',\n",
       " 'include',\n",
       " 'expect',\n",
       " 'identify',\n",
       " 'compare',\n",
       " 'develop',\n",
       " 'apply',\n",
       " 'increase',\n",
       " 'maintain']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_kcore_verbs(text,4,True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# graph of words\n",
    "### referencia  http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=907E7B3B0E507B2DDF531A314D215114?doi=10.1.1.348.6172&rep=rep1&type=pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from nltk import sent_tokenize\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import random\n",
    "import operator\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords # stopwords de nltk \n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean text - elegimos si filtramos los textos para dejar unicamente sustantivos (comunes + propios) y adjetivos o si filtramos solo stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter_nouns_adj indica el tipo de filtrado\n",
    "# True dejamos sustantivos (comunes y propios) y adjetivos\n",
    "# False filtramos solo stopwords\n",
    "\n",
    "def clean(text,filter_nouns_adj):\n",
    "    sentences=sent_tokenize(text)\n",
    "    sentences=[nlp(sentence) for sentence in sentences]\n",
    "    \n",
    "    if filter_nouns_adj==True:\n",
    "        sentences=[[token.lemma_ for token in sentence if token.pos_=='NOUN' or token.pos_=='ADJ' or token.pos_=='PROPN'] for sentence in sentences]\n",
    "    else:\n",
    "        sentences=[[token.lemma_ for token in sentence] for sentence in sentences]\n",
    "\n",
    "    text=[item for sublist in sentences for item in sublist]\n",
    "    text=[word for word in text if not word in stop_words]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat', 'dog', 'knife', 'man', 'woman', 'crocodile', 'lion', 'cat', 'hummer']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text='the cat killed the dog with a knife hello man woman crocodile. The lion killed the cat with a hummer'\n",
    "clean(text,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### graph of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K es el largo de la ventana\n",
    "# filter_nouns_adj indica el tipo de filtrado\n",
    "def graph_weighted(text,K,filter_nouns_adj):\n",
    "    text=text.replace('\\xa0','-')\n",
    "    text=clean(text,filter_nouns_adj)\n",
    "    unique_words=list(set(text))\n",
    "    G=nx.Graph()\n",
    "    for word in unique_words:\n",
    "        G.add_node(word)\n",
    "    for word in text: ## recorremos el texto y encontramos los indices de todas las aparicions de word (index_word)\n",
    "        index_word=[index for index, value in enumerate(text) if value == word]\n",
    "        ## ahora buscamos las palabras vecinas en una ventana de largo K (hacia adelante)\n",
    "        for index in index_word:\n",
    "            for k in range(1,K+1):\n",
    "                if index+k in range(len(text)):\n",
    "                    if G.has_edge(text[index],text[index+k])==False:\n",
    "                        G.add_edge(text[index],text[index+k],weight=1)\n",
    "                    else:\n",
    "                        x=G[text[index]][text[index+k]]['weight']\n",
    "                        G[text[index]][text[index+k]]['weight']=x+1\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<networkx.classes.graph.Graph at 0x7f5b9c219e80>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_weighted(text,2,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords a partir de graph of words. Se utiliza pagerank como algoritmo para encontrar nodos importantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K es el largo de la ventana\n",
    "# filter_nouns_adj indica el tipo de filtrado\n",
    "# number_keywords indica el numero de keywords\n",
    "def keywords_pagerank(text,number_keywords,filter_nouns_adj,K):\n",
    "    G=graph_weighted(text,K,filter_nouns_adj)\n",
    "    keywords=nx.pagerank(G, alpha=0.85, weight='weight')\n",
    "    return list(list(zip(*sorted(keywords.items(), key=operator.itemgetter(1),reverse=True)))[0][:number_keywords])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat', 'knife', 'woman', 'lion']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_pagerank(text,4,True,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### corpus de keywords+papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open('corpus_cell.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_keywords={}\n",
    "for i in range(len(data)):\n",
    "    if len(data[i]['introduction'])>0 and len(data[i]['keywords'])>0:\n",
    "        data_keywords[i]=[data[i]['introduction'],data[i]['keywords']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "introduction_keywords={}\n",
    "\n",
    "for key in data_keywords.keys():\n",
    "    n=0\n",
    "    for w in data_keywords[key][1]:\n",
    "        if w.upper()!=w:\n",
    "            n=1\n",
    "            break\n",
    "    if n==1:\n",
    "        introduction_keywords[key]=[re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", ' '.join(data_keywords[key][0])),[w.lower() for w in data_keywords[key][1]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_keyword={}\n",
    "for key in introduction_keywords.keys():\n",
    "    keyword_keyword[key]=['; '.join(keywords_pagerank(introduction_keywords[key][0],15,True,4)),'; '.join(introduction_keywords[key][1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cd8; t cell; exhaustion; dysfunction; cancer; single-cell; crispr/cas9; gata-3; metallothioneins; zinc; tils; tumor'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_keyword[79][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "lista_keywords=[]\n",
    "for key in introduction_keywords.keys():\n",
    "    lista_keywords+=[keyword_keyword[key]]\n",
    "\n",
    "df = pd.DataFrame(lista_keywords)\n",
    "## columnas - prediccion + original\n",
    "df.to_csv('keywords.csv', index=False, header=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "introductions=[introduction_keywords[key][0] for key in introduction_keywords.keys()]\n",
    "df = pd.DataFrame(introductions)\n",
    "df.to_csv('introductions.csv', index=False, header=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

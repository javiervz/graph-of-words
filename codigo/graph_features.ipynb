{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# graph of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## definimos los grafos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import community\n",
    "import ast\n",
    "import re\n",
    "from nltk import sent_tokenize\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import random\n",
    "import operator\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords # stopwords de nltk \n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter_nouns_adj indica el tipo de filtrado\n",
    "# True dejamos sustantivos (comunes y propios) y adjetivos\n",
    "# False filtramos solo stopwords\n",
    "\n",
    "def clean(text,filter_nouns_adj):\n",
    "    sentences=sent_tokenize(text)\n",
    "    sentences=[nlp(sentence) for sentence in sentences]\n",
    "    \n",
    "    if filter_nouns_adj==True:\n",
    "        sentences=[[token.lemma_ for token in sentence if token.pos_=='NOUN' or token.pos_=='ADJ' or token.pos_=='PROPN'] for sentence in sentences]\n",
    "    else:\n",
    "        sentences=[[token.lemma_ for token in sentence] for sentence in sentences]\n",
    "\n",
    "    text=[item for sublist in sentences for item in sublist]\n",
    "    text=[word for word in text if not word in stop_words]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K es el largo de la ventana\n",
    "# filter_nouns_adj indica el tipo de filtrado\n",
    "def graph_weighted(text,K,filter_nouns_adj):\n",
    "    text=clean(text,filter_nouns_adj)\n",
    "    unique_words=list(set(text))\n",
    "    G=nx.Graph()\n",
    "    for word in unique_words:\n",
    "        G.add_node(word)\n",
    "    for word in text: ## recorremos el texto y encontramos los indices de todas las aparicions de word (index_word)\n",
    "        index_word=[index for index, value in enumerate(text) if value == word]\n",
    "        ## ahora buscamos las palabras vecinas en una ventana de largo K (hacia adelante)\n",
    "        for index in index_word:\n",
    "            for k in range(1,K+1):\n",
    "                if index+k in range(len(text)):\n",
    "                    if G.has_edge(text[index],text[index+k])==False:\n",
    "                        G.add_edge(text[index],text[index+k],weight=1)\n",
    "                    else:\n",
    "                        x=G[text[index]][text[index+k]]['weight']\n",
    "                        G[text[index]][text[index+k]]['weight']=x+1\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## datos de diferentes Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('omics_temp.json','r')\n",
    "data_omics=data.read()\n",
    "data_omics = ast.literal_eval(data_omics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA=[]\n",
    "\n",
    "for k in range(len(data_omics)):\n",
    "    n=len(data_omics[k]['sections'])\n",
    "    data_omics_k={}\n",
    "    for i in range(n):\n",
    "        title=data_omics[k]['sections'][i]['title']\n",
    "        if title!='Abstract' and title!='Keywords' and title!='References':\n",
    "            if len(data_omics[k]['sections'][i]['paragraphs'])>1:\n",
    "                data_omics_k[data_omics[k]['sections'][i]['title']]=[' '.join(data_omics[k]['sections'][i]['paragraphs'])]\n",
    "            else:\n",
    "                data_omics_k[data_omics[k]['sections'][i]['title']]=data_omics[k]['sections'][i]['paragraphs']\n",
    "    DATA+=[data_omics_k]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### papers completos Q4!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_papers_Q4=[' '.join([item for sublist in list(paper.values()) for item in sublist]) for paper in DATA]\n",
    "complete_papers_Q4=[re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", paper) for paper in complete_papers_Q4]\n",
    "complete_papers_Q4=[paper for paper in complete_papers_Q4 if len(paper)>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2302"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(complete_papers_Q4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('cell_full_xml.json','r')\n",
    "data_cell=data.read()\n",
    "data_cell = ast.literal_eval(data_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exctractparragraphs(data):\n",
    "   #titles is a string\n",
    "   outputlist = []\n",
    "   for i in data:\n",
    "       for j in i[u'sections']:\n",
    "            temp1 = j[u'paragraphs']\n",
    "            if len(temp1) > 0:\n",
    "                for k in temp1:\n",
    "                    outputlist.append(k)\n",
    "            else:\n",
    "                temp1 = j[u'subsections']\n",
    "                for k in temp1:\n",
    "                    temp2 = k[u'paragraphs']\n",
    "                    if len(k)> 0:\n",
    "                        for l in temp2:\n",
    "                            outputlist.append(l)\n",
    "   return outputlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_cell=[]\n",
    "for i in range(len(data_cell)):\n",
    "    DATA_cell+=[exctractparragraphs([data_cell[i]])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### papers completos Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_papers_cell=[' '.join(paper) for paper in DATA_cell]\n",
    "complete_papers_cell=[re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", paper) for paper in complete_papers_cell]\n",
    "complete_papers_cell=[paper for paper in complete_papers_cell if len(paper)>1]\n",
    "complete_papers_cell=[paper.replace('__REF','') for paper in complete_papers_cell]\n",
    "complete_papers_cell=[paper.replace('REF__','') for paper in complete_papers_cell]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2118"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(complete_papers_cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rasgos de los grafos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### primero calculamos los grafos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "n=multiprocessing.cpu_count()\n",
    "#grafos_omics=[graph_weighted(paper,4,False) for paper in complete_papers_Q4]\n",
    "grafos_omics=Parallel(n_jobs=n)(delayed(graph_weighted)(paper,4,False) for paper in complete_papers_Q4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grafos_cell=Parallel(n_jobs=n)(delayed(graph_weighted)(paper,4,False) for paper in complete_papers_cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rasgos de los grafos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_features(graph):\n",
    "    \n",
    "    features={}\n",
    "    features['clustering']=nx.clustering(G, weight='weight') ## diccionario\n",
    "    features['betweenness_centrality']=nx.betweenness_centrality(G, normalized=False,weight='weight') ## diccionario\n",
    "    features['closeness_centrality']=nx.closeness_centrality(G) ## diccionario\n",
    "    fearures['core_number']=nx.core_number(G) ## diccionario\n",
    "    features['degree_centrality']=nx.degree_centrality(G) ## diccionario\n",
    "    features['diameter']=nx.diameter(G)\n",
    "    fearures['number_connected_components']=nx.number_connected_components(G)\n",
    "    partition = community.best_partition(G)\n",
    "    features['number_communities']=float(len(set(partition.values())))\n",
    "    features['number_triangles']=nx.triangles(G)/float(3)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keywords (W+J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import spacy\n",
    "import operator\n",
    "nlp = spacy.load('en')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from termcolor import colored\n",
    "import re\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "from collections import Counter\n",
    "data=open('stopwords.txt','r')\n",
    "data_read = data.read()\n",
    "stop_words=data_read.replace('\\n',' ').split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (0) funcion que limpia los textos\n",
    "### Esta funcion es mas general y entrega el texto filtrado y un diccionario de palabras filtradas {word original: word modificada,...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noun_adj == True significa que filtramos unicamente sustantivos y adjetivos, en otro caso, dejamos todo salvo ...\n",
    "def clean(text,noun_adj): ## filtramos unicamente los sustantivos y adjetivos\n",
    "    text=re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", text)\n",
    "    sentences=sent_tokenize(text)\n",
    "    sentences=[nlp(sentence.lower()) for sentence in sentences] ## lower() implica que se descartan los PROPN\n",
    "    if noun_adj==True:\n",
    "        sentences=[[(token.lemma_,token.text) for token in sentence if token.tag_=='NN' or token.tag_=='NNS' or token.tag_=='JJ'] for sentence in sentences]\n",
    "    else:\n",
    "        sentences=[[(token.lemma_,token.text) for token in sentence if token.lemma_ != '-PRON-' and token.is_punct==False and token.like_num==False] for sentence in sentences]\n",
    "    TEXT=[item for sublist in sentences for item in sublist]\n",
    "    TEXT=[(word_l,word_nl) for (word_l,word_nl) in TEXT if not word_l in stop_words or not word_nl in stop_words]\n",
    "    TEXT=[(re.sub(r'[^a-zA-Z0-9]', '', word_l),word_nl) for (word_l,word_nl) in TEXT]\n",
    "    TEXT=[(word_l,word_nl) for (word_l,word_nl) in TEXT if word_l.isdigit()==False and word_l!='' and word_nl.isdigit()==False and word_nl!='']\n",
    "    dict_text=list(set(TEXT))\n",
    "    dict_text={word_nl:word_l for (word_l,word_nl) in dict_text}\n",
    "    return list(zip(*TEXT))[0],dict_text ## texto filtrado, diccionario de word original: word modificada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('neanderthal',\n",
       " 'genome',\n",
       " 'denisovan',\n",
       " 'genome',\n",
       " 'early',\n",
       " 'modern',\n",
       " 'human',\n",
       " 'genome',\n",
       " 'eurasia',\n",
       " 'archaic',\n",
       " 'hominin',\n",
       " 'mark',\n",
       " 'genome',\n",
       " 'modern',\n",
       " 'human',\n",
       " 'present',\n",
       " 'day',\n",
       " 'individual',\n",
       " 'eurasia',\n",
       " 'genome',\n",
       " 'neanderthal',\n",
       " 'individual',\n",
       " 'oceania',\n",
       " 'genome',\n",
       " 'denisovan',\n",
       " 'suggestive',\n",
       " 'evidence',\n",
       " 'unidentified',\n",
       " 'hominin',\n",
       " 'specie',\n",
       " 'africa',\n",
       " 'functional',\n",
       " 'phenotypic',\n",
       " 'evolutionary',\n",
       " 'consequence',\n",
       " 'archaic',\n",
       " 'admixture',\n",
       " 'specific',\n",
       " 'haplotype',\n",
       " 'allele',\n",
       " 'archaic',\n",
       " 'hominin',\n",
       " 'ancestor',\n",
       " 'approach',\n",
       " 'introgressed',\n",
       " 'haplotype',\n",
       " 'method',\n",
       " 'reference',\n",
       " 'archaic',\n",
       " 'hominin',\n",
       " 'genome',\n",
       " 'sequence',\n",
       " 'reference',\n",
       " 'free',\n",
       " 'method',\n",
       " 'information',\n",
       " 'category',\n",
       " 'method',\n",
       " 'sankararaman',\n",
       " 'haplotype',\n",
       " 'modern',\n",
       " 'human',\n",
       " 'haplotype',\n",
       " 'reference',\n",
       " 'archaic',\n",
       " 'sequence',\n",
       " 'category',\n",
       " 'method',\n",
       " 's',\n",
       " 'statistic',\n",
       " 'mutational',\n",
       " 'signature',\n",
       " 'ancient',\n",
       " 'admixture',\n",
       " 'leaf',\n",
       " 'genome',\n",
       " 'present',\n",
       " 'day',\n",
       " 'human',\n",
       " 's',\n",
       " 'approach',\n",
       " 'powerful',\n",
       " 'introgressed',\n",
       " 'haplotype',\n",
       " 'absence',\n",
       " 'archaic',\n",
       " 'reference',\n",
       " 'genome',\n",
       " 'unusual',\n",
       " 'mutational',\n",
       " 'characteristic',\n",
       " 'introgressed',\n",
       " 'haplotype',\n",
       " 'long',\n",
       " 'divergence',\n",
       " 'time',\n",
       " 'neanderthal',\n",
       " 'modern',\n",
       " 'human',\n",
       " 'neanderthal',\n",
       " 'allele',\n",
       " 'specific',\n",
       " 'lineage',\n",
       " 'allele',\n",
       " 'present',\n",
       " 'introgressed',\n",
       " 'haplotype',\n",
       " 'absent',\n",
       " 'rare',\n",
       " 'african',\n",
       " 'genome',\n",
       " 'recent',\n",
       " 'timing',\n",
       " 'admixture',\n",
       " 'introgressed',\n",
       " 'haplotype',\n",
       " 'recombination',\n",
       " 'distance',\n",
       " 'kb',\n",
       " 'average',\n",
       " 'high',\n",
       " 'level',\n",
       " 'linkage',\n",
       " 'disequilibrium',\n",
       " 'neanderthal',\n",
       " 'specific',\n",
       " 'allele',\n",
       " 'african',\n",
       " 'human',\n",
       " 'genome',\n",
       " 'study',\n",
       " 'slike',\n",
       " 'method',\n",
       " 'power',\n",
       " 'suitable',\n",
       " 'large',\n",
       " 'scale',\n",
       " 'genome',\n",
       " 'wide',\n",
       " 'datum',\n",
       " 'method',\n",
       " 'large',\n",
       " 'set',\n",
       " 'datum',\n",
       " 'eurasia',\n",
       " 'oceania',\n",
       " 'putative',\n",
       " 'archaic',\n",
       " 'specific',\n",
       " 'allele',\n",
       " 'rate',\n",
       " 'allele',\n",
       " 'archaic',\n",
       " 'genome',\n",
       " 'role',\n",
       " 'gene',\n",
       " 'allele',\n",
       " 'insight',\n",
       " 'history',\n",
       " 'admixture',\n",
       " 'event',\n",
       " 'impact',\n",
       " 'modern',\n",
       " 'human',\n",
       " 'genome')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text='Sequencing the Neanderthal genome (Green et al., 2010, Prüfer et al., 2014), the Denisovan genome (Reich et al., 2010), and several early modern human genomes from Eurasia (Fu et al., 2014, Fu et al., 2015) has confirmed that archaic hominins left their mark in the genomes of modern humans (Plagnol and Wall, 2006, Sankararaman et al., 2014, Vernot and Akey, 2014, Vernot et al., 2016). Present-day individuals in Eurasia inherited ∼2% of their genome from Neanderthals (Green et al., 2010), and individuals from Oceania inherited ∼5% of their genome from Denisovans (Reich et al., 2010). Suggestive evidence indicates that admixture from other unidentified hominin species occurred in Africa (Hammer et al., 2011, Hsieh et al., 2016, Lachance et al., 2012, Plagnol and Wall, 2006, Wall et al., 2009). To understand the functional, phenotypic, and evolutionary consequences of archaic admixture, it is necessary to identify the specific haplotypes and alleles that were inherited from archaic hominin ancestors (Huerta-Sánchez et al., 2014, Juric et al., 2016, Sankararaman et al., 2014, Simonti et al., 2016, Vernot and Akey, 2014). Approaches to identifying introgressed haplotypes include methods that specifically incorporate reference archaic hominin genome sequences and reference-free methods that do not utilize such information. An example of the former category is the method of Sankararaman et al. (2014), which identifies archaic haplotypes by comparing modern human haplotypes to a reference archaic sequence. The latter category of methods include the S∗ statistic (Plagnol and Wall, 2006), which searches for the mutational signature that ancient admixture leaves in the genomes of present-day humans. The S∗ approach is powerful for finding introgressed haplotypes in the absence of an archaic reference genome because it leverages the unusual mutational characteristics of introgressed haplotypes. Because of the long divergence time between Neanderthals and modern humans, Neanderthals carry many alleles that are specific to their lineage. Such alleles are present on introgressed haplotypes but are absent or rare in African genomes. Further, based on the recent timing of admixture, introgressed haplotypes are expected to be maintained without recombination over distances of approximately 50 kb on average (Sankararaman et al., 2012), resulting in high levels of linkage disequilibrium (LD) between Neanderthal-specific alleles in non-African human genomes. In this study, we develop an S∗-like method that has increased power and is suitable for large-scale genome-wide data. We apply the method to large sets of sequenced data from Eurasia and Oceania and identify putative archaic-specific alleles. We examine the rate at which these alleles match the sequenced archaic genomes and the role of the genes containing these alleles, to obtain insights into the history of the admixture events and their impact on modern human genomes.'\n",
    "text_clean,dict_words=clean(text,True)\n",
    "text_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) grafo de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K es el largo de la ventana\n",
    "# text_clean es una lista de palabras de un texto ya procesado por clean\n",
    "# digraph indica el tipo de grafo- True = dirigido, False = no dirigido\n",
    "def graph_weighted(text_clean,K,digraph):\n",
    "    unique_words=list(set(text_clean))\n",
    "    if digraph==True: ## grafo dirigido o no dirigido\n",
    "        G=nx.DiGraph()\n",
    "    else:\n",
    "        G=nx.Graph()\n",
    "    for word in unique_words:\n",
    "        G.add_node(word)\n",
    "    for word in unique_words: ## recorremos el texto y encontramos los indices de todas las aparicions de word (index_word)\n",
    "        index_word=[index for index, value in enumerate(text_clean) if value == word]\n",
    "        ## ahora buscamos las palabras vecinas en una ventana de largo K (hacia adelante)\n",
    "        for index in index_word:\n",
    "            for k in range(1,K+1):\n",
    "                if index+k in range(len(text_clean)):\n",
    "                    if G.has_edge(text_clean[index],text_clean[index+k])==False:\n",
    "                        G.add_edge(text_clean[index],text_clean[index+k],weight=1)\n",
    "                    else:\n",
    "                        x=G[text_clean[index]][text_clean[index+k]]['weight']\n",
    "                        G[text_clean[index]][text_clean[index+k]]['weight']=x+1\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<networkx.classes.digraph.DiGraph at 0x7f3d2f351c18>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_weighted(text_clean,8,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) keywords segun pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K es el largo de la ventana\n",
    "# text_clean es una lista de palabras de un texto ya procesado por clean\n",
    "# number_keywords indica el numero de keywords\n",
    "# digraph indica si queremos un grafo dirigido (True) o no dirigido (False)\n",
    "def keywords_pagerank(text_clean,number_keywords,K,digraph):\n",
    "    n=len(text_clean)\n",
    "    G=graph_weighted(text_clean,K,digraph)\n",
    "    keywords=nx.pagerank(G, alpha=0.85, weight='weight')\n",
    "    if n<number_keywords: ## en el caso de que el texto sea muy corto (incluso menor al numero de keywords)\n",
    "        number_keywords=n\n",
    "    return list(list(zip(*sorted(keywords.items(), key=operator.itemgetter(1),reverse=True)))[0][:number_keywords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['genome',\n",
       " 'haplotype',\n",
       " 'archaic',\n",
       " 'human',\n",
       " 'allele',\n",
       " 'method',\n",
       " 'modern',\n",
       " 'introgressed',\n",
       " 'specific',\n",
       " 'neanderthal']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_pagerank(text_clean,10,8,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) keywords segun main core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K es el largo de la ventana\n",
    "# text_clean es una lista de palabras de un texto ya procesado por clean\n",
    "# digraph es el tipo de grafo - True = grafo dirigido, False = grafo no dirigido\n",
    "# en esta funcion, no es necesario indicar el numero de keywords. Fijamos un maximo de number_keywords\n",
    "def keywords_kcore(text_clean,number_keywords,K,digraph):\n",
    "    G=graph_weighted(text_clean,K,digraph)\n",
    "    G.remove_edges_from(nx.selfloop_edges(G)) ## borramos ciclos para evitar que Networkx entregue un error\n",
    "    main_core_nodes=list(nx.k_core(G).nodes())\n",
    "    n=len(main_core_nodes)\n",
    "    if n>number_keywords:\n",
    "        return main_core_nodes[:number_keywords]\n",
    "    else:\n",
    "        return main_core_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['present',\n",
       " 'genome',\n",
       " 'day',\n",
       " 'neanderthal',\n",
       " 'hominin',\n",
       " 'mutational',\n",
       " 'category',\n",
       " 'method',\n",
       " 'allele',\n",
       " 's']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_kcore(text_clean,10,8,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Main core rankeado segun Core Rank\n",
    "### http://www.lix.polytechnique.fr/~anti5662/eacl_17_real_time_meladianos_tixier_nikolentzos_vazirgiannis.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K es el largo de la ventana\n",
    "# text_clean es una lista de palabras de un texto ya procesado por clean\n",
    "# digraph es el tipo de grafo - True = grafo dirigido, False = grafo no dirigido\n",
    "def keywords_kcore_rank(text_clean,number_keywords,K,digraph):\n",
    "    G=graph_weighted(text_clean,K,digraph)\n",
    "    G.remove_edges_from(nx.selfloop_edges(G)) ## borramos ciclos para evitar que Networkx entregue un error\n",
    "    core_number=nx.core_number(G) ## core number de los nodos de G\n",
    "    core_rank={word:sum([core_number[w] for w in list(dict(G[word]).keys())]) for word in list(core_number.keys())}\n",
    "    main_core_nodes=list(nx.k_core(G).nodes())\n",
    "    ## ahora vemos el core rank de los nodos del main core\n",
    "    main_core_rank=[]\n",
    "    for node in main_core_nodes:\n",
    "        main_core_rank+=[(node,core_rank[node])]\n",
    "    ## ordenamos segun core rank decreciente\n",
    "    main_core_rank=sorted(main_core_rank, key=lambda tup: tup[1],reverse=True)\n",
    "    main_core_rank=list(zip(*main_core_rank))[0]\n",
    "    ## elegimos number_keywords keywords\n",
    "    n=len(main_core_rank)\n",
    "    if n>number_keywords:\n",
    "        return list(main_core_rank[:number_keywords])\n",
    "    else:\n",
    "        return list(main_core_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['genome',\n",
       " 'haplotype',\n",
       " 'archaic',\n",
       " 'human',\n",
       " 'method',\n",
       " 'introgressed',\n",
       " 'allele',\n",
       " 'hominin',\n",
       " 'admixture',\n",
       " 'neanderthal']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_kcore_rank(text_clean,10,8,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) n-gramas de keywords\n",
    "## Con el conjunto de palabras del main core, revisamos el texto original en busqueda de n-gramas de palabras adyacentes. De esta forma, se extraen ngramas 'importantes', que sean representativos de las ideas centrales del texto. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K es el largo de la ventana\n",
    "# text es el texto original\n",
    "# text_clean es una lista de palabras de un texto ya procesado por clean\n",
    "# dict_words es el diccionario que entrega clean\n",
    "# digraph es el tipo de grafo - True = grafo dirigido, False = grafo no dirigido\n",
    "# lemmatization es True o False\n",
    "def main_core_ngrams(text,text_clean,dict_words,K,digraph,lemmatization):\n",
    "    G=graph_weighted(text_clean,K,digraph)\n",
    "    G.remove_edges_from(nx.selfloop_edges(G)) \n",
    "    d=nx.core_number(G) ## diccionario de palabras asociadas a un core\n",
    "    max_core=max(d.values())\n",
    "    text=[w.lower() for w in word_tokenize(text)]\n",
    "    c=[]\n",
    "    for word in text:\n",
    "        if word in dict_words.keys():\n",
    "            if d[dict_words[word]]==max_core:\n",
    "                if lemmatization==True:\n",
    "                    c+=[dict_words[word]]\n",
    "                else:\n",
    "                    c+=[word]\n",
    "            else:\n",
    "                c+=['X']\n",
    "        else:\n",
    "            c+=['X']\n",
    "    s=' '.join(c)\n",
    "    ngrams=[token.rstrip().lstrip() for token in s.split('X') if token!=' ' and token!='']\n",
    "    L=[[len(word_tokenize(token)),token] for token in ngrams]\n",
    "    o = OrderedDict()\n",
    "    for x in L:\n",
    "        o.setdefault(x[0], []).append(x[1])\n",
    "    for key in o.keys():\n",
    "        if len(o[key])>1:\n",
    "            o[key]=list(list(zip(*Counter(o[key]).most_common()))[0])\n",
    "        \n",
    "    return dict(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: ['allele',\n",
       "  'genome',\n",
       "  'method',\n",
       "  'admixture',\n",
       "  'eurasia',\n",
       "  'neanderthal',\n",
       "  'category',\n",
       "  'mutational',\n",
       "  'hominin',\n",
       "  'approach',\n",
       "  's',\n",
       "  'human',\n",
       "  'specific',\n",
       "  'present'],\n",
       " 2: ['introgressed haplotype',\n",
       "  'archaic hominin',\n",
       "  'modern human',\n",
       "  'neanderthal genome',\n",
       "  'archaic admixture',\n",
       "  'specific haplotype',\n",
       "  'archaic haplotype',\n",
       "  's approach',\n",
       "  'human genome',\n",
       "  'archaic genome'],\n",
       " 3: ['modern human genome',\n",
       "  'modern human haplotype',\n",
       "  'reference archaic sequence',\n",
       "  'archaic reference genome'],\n",
       " 5: ['reference archaic hominin genome sequence']}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_core_ngrams(text,text_clean,dict_words,8,True,True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (6) n-gramas de keywords para diferentes valores de k (cores)\n",
    "## Para cada k (core), extraemos el equivalente de la funcion (5). Con esto, formamos un diccionario, en el cual las keys son k y los values son diccionarios al modo de la funcion (5).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K es el largo de la ventana\n",
    "# text es el texto original\n",
    "# text_clean es una lista de palabras de un texto ya procesado por clean\n",
    "# dict_words es el diccionario que entrega clean\n",
    "# digraph es el tipo de grafo - True = grafo dirigido, False = grafo no dirigido\n",
    "def k_core_ngrams(text,text_clean,dict_words,K,digraph,lemmatization):\n",
    "    G=graph_weighted(text_clean,K,digraph)\n",
    "    G.remove_edges_from(nx.selfloop_edges(G)) \n",
    "    d=nx.core_number(G)\n",
    "    k_core={}\n",
    "    text=[w.lower() for w in word_tokenize(text)]\n",
    "    for core in d.values():\n",
    "        c=[]\n",
    "        for word in text: ## con este for rodeamos las palabras del k core con 'X'\n",
    "            if word in dict_words.keys():\n",
    "                if d[dict_words[word]]==core:\n",
    "                    if lemmatization==True:\n",
    "                        c+=[dict_words[word]]\n",
    "                    else:\n",
    "                        c+=[word]\n",
    "                else:\n",
    "                    c+=['X']\n",
    "            else:\n",
    "                c+=['X']\n",
    "        s=' '.join(c)\n",
    "        ngrams=[token.rstrip().lstrip() for token in s.split('X') if token!=' ' and token!=''] ## split con 'X'\n",
    "        L=[[len(word_tokenize(token)),token] for token in ngrams]\n",
    "        o = OrderedDict()\n",
    "        for x in L:\n",
    "            o.setdefault(x[0], []).append(x[1])\n",
    "        for key in o.keys():\n",
    "            if len(o[key])>1:\n",
    "                o[key]=list(list(zip(*Counter(o[key]).most_common()))[0]) ## ordenamos los resultados x frecuencia\n",
    "    \n",
    "        k_core[core]=dict(o)\n",
    "        \n",
    "    return k_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{10: {1: ['early']},\n",
       " 11: {1: ['putative',\n",
       "   'rate',\n",
       "   'role',\n",
       "   'gene',\n",
       "   'insight',\n",
       "   'history',\n",
       "   'event',\n",
       "   'impact']},\n",
       " 13: {1: ['datum', 'mark', 'study', 'slike', 'power', 'suitable'],\n",
       "  2: ['large set']},\n",
       " 14: {1: ['sankararaman',\n",
       "   'denisovan',\n",
       "   'individual',\n",
       "   'oceania',\n",
       "   'unidentified',\n",
       "   'specie',\n",
       "   'africa',\n",
       "   'functional',\n",
       "   'phenotypic',\n",
       "   'ancestor',\n",
       "   'information',\n",
       "   'powerful',\n",
       "   'absence',\n",
       "   'unusual',\n",
       "   'characteristic',\n",
       "   'lineage',\n",
       "   'absent',\n",
       "   'rare',\n",
       "   'african',\n",
       "   'recombination',\n",
       "   'distance',\n",
       "   'kb',\n",
       "   'average'],\n",
       "  2: ['suggestive evidence',\n",
       "   'evolutionary consequence',\n",
       "   'recent timing',\n",
       "   'high level',\n",
       "   'linkage disequilibrium'],\n",
       "  3: ['long divergence time']},\n",
       " 15: {1: ['statistic', 'signature', 'ancient', 'leaf']},\n",
       " 17: {1: ['allele',\n",
       "   'genome',\n",
       "   'method',\n",
       "   'admixture',\n",
       "   'eurasia',\n",
       "   'neanderthal',\n",
       "   'category',\n",
       "   'mutational',\n",
       "   'hominin',\n",
       "   'approach',\n",
       "   's',\n",
       "   'human',\n",
       "   'specific',\n",
       "   'present'],\n",
       "  2: ['introgressed haplotype',\n",
       "   'archaic hominin',\n",
       "   'modern human',\n",
       "   'neanderthal genome',\n",
       "   'archaic admixture',\n",
       "   'specific haplotype',\n",
       "   'archaic haplotype',\n",
       "   's approach',\n",
       "   'human genome',\n",
       "   'archaic genome'],\n",
       "  3: ['modern human genome',\n",
       "   'modern human haplotype',\n",
       "   'reference archaic sequence',\n",
       "   'archaic reference genome'],\n",
       "  5: ['reference archaic hominin genome sequence']}}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_core_ngrams(text,text_clean,dict_words,8,True,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (7) ahora extraemos ngramas que unen k-core con (k-1)-core\n",
    "## En esta nueva funcion, se extiende (6) con el fin de incluir n-gramas formados por palabras que aparezcan en el main-core o en el (main-1)-core. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text es el texto original\n",
    "# text_clean es una lista de palabras de un texto ya procesado por clean\n",
    "# dict_words es el diccionario que entrega clean\n",
    "# digraph es el tipo de grafo - True = grafo dirigido, False = grafo no dirigido\n",
    "def main_core_combination_ngrams(text,text_clean,dict_words,K,digraph,lemmatization):\n",
    "    G=graph_weighted(text_clean,K,digraph)\n",
    "    G.remove_edges_from(nx.selfloop_edges(G)) \n",
    "    d=nx.core_number(G) ## diccionario de palabras asociadas a un core\n",
    "    max_core=max(d.values())\n",
    "    max_core_minus=sorted(list(set(d.values())),reverse=True)[1] ## el segundo core mas grande\n",
    "    text=[w.lower() for w in word_tokenize(text)]\n",
    "    c=[]\n",
    "    for word in text:\n",
    "        if word in dict_words.keys():\n",
    "            if d[dict_words[word]]==max_core:\n",
    "                if lemmatization==True:\n",
    "                    c+=[dict_words[word]]\n",
    "                else:\n",
    "                    c+=[word]\n",
    "            elif d[dict_words[word]]==max_core_minus:\n",
    "                if lemmatization==True:\n",
    "                    c+=[dict_words[word]]\n",
    "                else:\n",
    "                    c+=[word]\n",
    "            else:\n",
    "                c+=['X']\n",
    "        else:\n",
    "            c+=['X']\n",
    "    s=' '.join(c)\n",
    "    ngrams=[token.rstrip().lstrip() for token in s.split('X') if token!=' ' and token!='']\n",
    "    L=[[len(word_tokenize(token)),token] for token in ngrams]\n",
    "    o = OrderedDict()\n",
    "    for x in L:\n",
    "        o.setdefault(x[0], []).append(x[1])\n",
    "    for key in o.keys():\n",
    "        if len(o[key])>1:\n",
    "            o[key]=list(list(zip(*Counter(o[key]).most_common()))[0])\n",
    "        \n",
    "    return dict(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: ['allele',\n",
       "  'genome',\n",
       "  'method',\n",
       "  'eurasia',\n",
       "  'neanderthal',\n",
       "  'admixture',\n",
       "  'category',\n",
       "  'hominin',\n",
       "  'approach',\n",
       "  'human',\n",
       "  'mutational',\n",
       "  'specific',\n",
       "  'present'],\n",
       " 2: ['introgressed haplotype',\n",
       "  'archaic hominin',\n",
       "  'modern human',\n",
       "  'neanderthal genome',\n",
       "  'archaic admixture',\n",
       "  'specific haplotype',\n",
       "  'archaic haplotype',\n",
       "  's statistic',\n",
       "  'mutational signature',\n",
       "  's approach',\n",
       "  'human genome',\n",
       "  'archaic genome'],\n",
       " 3: ['modern human genome',\n",
       "  'modern human haplotype',\n",
       "  'reference archaic sequence',\n",
       "  'ancient admixture leaf',\n",
       "  'archaic reference genome'],\n",
       " 5: ['reference archaic hominin genome sequence']}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_core_combination_ngrams(text,text_clean,dict_words,8,True,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

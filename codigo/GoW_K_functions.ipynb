{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keywords (W+J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import spacy\n",
    "import operator\n",
    "nlp = spacy.load('en')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from termcolor import colored\n",
    "import re\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "from collections import Counter\n",
    "data=open('stopwords.txt','r')\n",
    "data_read = data.read()\n",
    "stop_words=data_read.replace('\\n',' ').split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (0) funcion que limpia los textos\n",
    "### Esta funcion es mas general y entrega el texto filtrado y un diccionario de las palabras filtradas {word original: word modificada,...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    text=re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", text)\n",
    "    sentences=sent_tokenize(text)\n",
    "    sentences=[nlp(sentence.lower()) for sentence in sentences] ## lower() implica que se descartan los PROPN\n",
    "    sentences=[[(token.lemma_,token.text) for token in sentence if token.tag_=='NN' or token.tag_=='NNS' or token.tag_=='JJ'] for sentence in sentences]\n",
    "    TEXT=[item for sublist in sentences for item in sublist]\n",
    "    TEXT=[(word_l,word_nl) for (word_l,word_nl) in TEXT if not word_l in stop_words or not word_nl in stop_words]\n",
    "    TEXT=[(re.sub(r'[^a-zA-Z0-9]', '', word_l),word_nl) for (word_l,word_nl) in TEXT]\n",
    "    TEXT=[(word_l,word_nl) for (word_l,word_nl) in TEXT if word_l.isdigit()==False and word_l!='' and word_nl.isdigit()==False and word_nl!='']\n",
    "    dict_text=list(set(TEXT))\n",
    "    dict_text={word_nl:word_l for (word_l,word_nl) in dict_text}\n",
    "    return list(zip(*TEXT))[0],dict_text ## texto filtrado, diccionario de word original: word modificada\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('neanderthal',\n",
       " 'genome',\n",
       " 'denisovan',\n",
       " 'genome',\n",
       " 'early',\n",
       " 'modern',\n",
       " 'human',\n",
       " 'genome',\n",
       " 'eurasia',\n",
       " 'archaic',\n",
       " 'hominin',\n",
       " 'mark',\n",
       " 'genome',\n",
       " 'modern',\n",
       " 'human',\n",
       " 'present',\n",
       " 'day',\n",
       " 'individual',\n",
       " 'eurasia',\n",
       " 'genome',\n",
       " 'neanderthal',\n",
       " 'individual',\n",
       " 'oceania',\n",
       " 'genome',\n",
       " 'denisovan',\n",
       " 'suggestive',\n",
       " 'evidence',\n",
       " 'unidentified',\n",
       " 'hominin',\n",
       " 'specie',\n",
       " 'africa',\n",
       " 'functional',\n",
       " 'phenotypic',\n",
       " 'evolutionary',\n",
       " 'consequence',\n",
       " 'archaic',\n",
       " 'admixture',\n",
       " 'specific',\n",
       " 'haplotype',\n",
       " 'allele',\n",
       " 'archaic',\n",
       " 'hominin',\n",
       " 'ancestor',\n",
       " 'approach',\n",
       " 'introgressed',\n",
       " 'haplotype',\n",
       " 'method',\n",
       " 'reference',\n",
       " 'archaic',\n",
       " 'hominin',\n",
       " 'genome',\n",
       " 'sequence',\n",
       " 'reference',\n",
       " 'free',\n",
       " 'method',\n",
       " 'information',\n",
       " 'category',\n",
       " 'method',\n",
       " 'sankararaman',\n",
       " 'haplotype',\n",
       " 'modern',\n",
       " 'human',\n",
       " 'haplotype',\n",
       " 'reference',\n",
       " 'archaic',\n",
       " 'sequence',\n",
       " 'category',\n",
       " 'method',\n",
       " 's',\n",
       " 'statistic',\n",
       " 'mutational',\n",
       " 'signature',\n",
       " 'ancient',\n",
       " 'admixture',\n",
       " 'leaf',\n",
       " 'genome',\n",
       " 'present',\n",
       " 'day',\n",
       " 'human',\n",
       " 's',\n",
       " 'approach',\n",
       " 'powerful',\n",
       " 'introgressed',\n",
       " 'haplotype',\n",
       " 'absence',\n",
       " 'archaic',\n",
       " 'reference',\n",
       " 'genome',\n",
       " 'unusual',\n",
       " 'mutational',\n",
       " 'characteristic',\n",
       " 'introgressed',\n",
       " 'haplotype',\n",
       " 'long',\n",
       " 'divergence',\n",
       " 'time',\n",
       " 'neanderthal',\n",
       " 'modern',\n",
       " 'human',\n",
       " 'neanderthal',\n",
       " 'allele',\n",
       " 'specific',\n",
       " 'lineage',\n",
       " 'allele',\n",
       " 'present',\n",
       " 'introgressed',\n",
       " 'haplotype',\n",
       " 'absent',\n",
       " 'rare',\n",
       " 'african',\n",
       " 'genome',\n",
       " 'recent',\n",
       " 'timing',\n",
       " 'admixture',\n",
       " 'introgressed',\n",
       " 'haplotype',\n",
       " 'recombination',\n",
       " 'distance',\n",
       " 'kb',\n",
       " 'average',\n",
       " 'high',\n",
       " 'level',\n",
       " 'linkage',\n",
       " 'disequilibrium',\n",
       " 'neanderthal',\n",
       " 'specific',\n",
       " 'allele',\n",
       " 'african',\n",
       " 'human',\n",
       " 'genome',\n",
       " 'study',\n",
       " 'slike',\n",
       " 'method',\n",
       " 'power',\n",
       " 'suitable',\n",
       " 'large',\n",
       " 'scale',\n",
       " 'genome',\n",
       " 'wide',\n",
       " 'datum',\n",
       " 'method',\n",
       " 'large',\n",
       " 'set',\n",
       " 'datum',\n",
       " 'eurasia',\n",
       " 'oceania',\n",
       " 'putative',\n",
       " 'archaic',\n",
       " 'specific',\n",
       " 'allele',\n",
       " 'rate',\n",
       " 'allele',\n",
       " 'archaic',\n",
       " 'genome',\n",
       " 'role',\n",
       " 'gene',\n",
       " 'allele',\n",
       " 'insight',\n",
       " 'history',\n",
       " 'admixture',\n",
       " 'event',\n",
       " 'impact',\n",
       " 'modern',\n",
       " 'human',\n",
       " 'genome')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text='Sequencing the Neanderthal genome (Green et al., 2010, Prüfer et al., 2014), the Denisovan genome (Reich et al., 2010), and several early modern human genomes from Eurasia (Fu et al., 2014, Fu et al., 2015) has confirmed that archaic hominins left their mark in the genomes of modern humans (Plagnol and Wall, 2006, Sankararaman et al., 2014, Vernot and Akey, 2014, Vernot et al., 2016). Present-day individuals in Eurasia inherited ∼2% of their genome from Neanderthals (Green et al., 2010), and individuals from Oceania inherited ∼5% of their genome from Denisovans (Reich et al., 2010). Suggestive evidence indicates that admixture from other unidentified hominin species occurred in Africa (Hammer et al., 2011, Hsieh et al., 2016, Lachance et al., 2012, Plagnol and Wall, 2006, Wall et al., 2009). To understand the functional, phenotypic, and evolutionary consequences of archaic admixture, it is necessary to identify the specific haplotypes and alleles that were inherited from archaic hominin ancestors (Huerta-Sánchez et al., 2014, Juric et al., 2016, Sankararaman et al., 2014, Simonti et al., 2016, Vernot and Akey, 2014). Approaches to identifying introgressed haplotypes include methods that specifically incorporate reference archaic hominin genome sequences and reference-free methods that do not utilize such information. An example of the former category is the method of Sankararaman et al. (2014), which identifies archaic haplotypes by comparing modern human haplotypes to a reference archaic sequence. The latter category of methods include the S∗ statistic (Plagnol and Wall, 2006), which searches for the mutational signature that ancient admixture leaves in the genomes of present-day humans. The S∗ approach is powerful for finding introgressed haplotypes in the absence of an archaic reference genome because it leverages the unusual mutational characteristics of introgressed haplotypes. Because of the long divergence time between Neanderthals and modern humans, Neanderthals carry many alleles that are specific to their lineage. Such alleles are present on introgressed haplotypes but are absent or rare in African genomes. Further, based on the recent timing of admixture, introgressed haplotypes are expected to be maintained without recombination over distances of approximately 50 kb on average (Sankararaman et al., 2012), resulting in high levels of linkage disequilibrium (LD) between Neanderthal-specific alleles in non-African human genomes. In this study, we develop an S∗-like method that has increased power and is suitable for large-scale genome-wide data. We apply the method to large sets of sequenced data from Eurasia and Oceania and identify putative archaic-specific alleles. We examine the rate at which these alleles match the sequenced archaic genomes and the role of the genes containing these alleles, to obtain insights into the history of the admixture events and their impact on modern human genomes.'\n",
    "text_clean,dict_words=clean(text)\n",
    "text_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) grafo de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K es el largo de la ventana\n",
    "# text_clean es una lista de palabras de un texto ya procesado por clean\n",
    "# digraph indica el tipo de grafo- True = dirigido, False = no dirigido\n",
    "def graph_weighted(text_clean,K,digraph):\n",
    "    unique_words=list(set(text_clean))\n",
    "    if digraph==True: ## grafo dirigido o no dirigido\n",
    "        G=nx.DiGraph()\n",
    "    else:\n",
    "        G=nx.Graph()\n",
    "    for word in unique_words:\n",
    "        G.add_node(word)\n",
    "    for word in unique_words: ## recorremos el texto y encontramos los indices de todas las aparicions de word (index_word)\n",
    "        index_word=[index for index, value in enumerate(text_clean) if value == word]\n",
    "        ## ahora buscamos las palabras vecinas en una ventana de largo K (hacia adelante)\n",
    "        for index in index_word:\n",
    "            for k in range(1,K+1):\n",
    "                if index+k in range(len(text_clean)):\n",
    "                    if G.has_edge(text_clean[index],text_clean[index+k])==False:\n",
    "                        G.add_edge(text_clean[index],text_clean[index+k],weight=1)\n",
    "                    else:\n",
    "                        x=G[text_clean[index]][text_clean[index+k]]['weight']\n",
    "                        G[text_clean[index]][text_clean[index+k]]['weight']=x+1\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<networkx.classes.digraph.DiGraph at 0x7fb735eadf98>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_weighted(text_clean,8,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) keywords segun pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K es el largo de la ventana\n",
    "# text_clean es una lista de palabras de un texto ya procesado por clean\n",
    "# number_keywords indica el numero de keywords\n",
    "# digraph indica si queremos un grafo dirigido (True) o no dirigido (False)\n",
    "def keywords_pagerank(text_clean,number_keywords,K,digraph):\n",
    "    n=len(text_clean)\n",
    "    G=graph_weighted(text_clean,K,digraph)\n",
    "    keywords=nx.pagerank(G, alpha=0.85, weight='weight')\n",
    "    if n<number_keywords: ## en el caso de que el texto sea muy corto (incluso menor al numero de keywords)\n",
    "        number_keywords=n\n",
    "    return list(list(zip(*sorted(keywords.items(), key=operator.itemgetter(1),reverse=True)))[0][:number_keywords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['genome',\n",
       " 'haplotype',\n",
       " 'archaic',\n",
       " 'human',\n",
       " 'allele',\n",
       " 'method',\n",
       " 'modern',\n",
       " 'introgressed',\n",
       " 'specific',\n",
       " 'neanderthal']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_pagerank(text_clean,10,8,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) keywords segun main core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K es el largo de la ventana\n",
    "# text_clean es una lista de palabras de un texto ya procesado por clean\n",
    "# digraph es el tipo de grafo - True = grafo dirigido, False = grafo no dirigido\n",
    "# en esta funcion, no es necesario indicar el numero de keywords. Fijamos un maximo de number_keywords\n",
    "def keywords_kcore(text_clean,number_keywords,K,digraph):\n",
    "    G=graph_weighted(text_clean,K,digraph)\n",
    "    G.remove_edges_from(nx.selfloop_edges(G)) ## borramos ciclos para evitar que Networkx entregue un error\n",
    "    main_core_nodes=list(nx.k_core(G).nodes())\n",
    "    n=len(main_core_nodes)\n",
    "    if n>number_keywords:\n",
    "        return main_core_nodes[:number_keywords]\n",
    "    else:\n",
    "        return main_core_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['genome',\n",
       " 'present',\n",
       " 'reference',\n",
       " 'method',\n",
       " 'haplotype',\n",
       " 'modern',\n",
       " 'neanderthal',\n",
       " 'admixture',\n",
       " 'archaic',\n",
       " 'category']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_kcore(text_clean,10,8,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Main core rankeado segun Core Rank\n",
    "### http://www.lix.polytechnique.fr/~anti5662/eacl_17_real_time_meladianos_tixier_nikolentzos_vazirgiannis.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K es el largo de la ventana\n",
    "# text_clean es una lista de palabras de un texto ya procesado por clean\n",
    "# digraph es el tipo de grafo - True = grafo dirigido, False = grafo no dirigido\n",
    "def keywords_kcore_rank(text_clean,number_keywords,K,digraph):\n",
    "    G=graph_weighted(text_clean,K,digraph)\n",
    "    G.remove_edges_from(nx.selfloop_edges(G)) ## borramos ciclos para evitar que Networkx entregue un error\n",
    "    core_number=nx.core_number(G) ## core number de los nodos de G\n",
    "    core_rank={word:sum([core_number[w] for w in list(dict(G[word]).keys())]) for word in list(core_number.keys())}\n",
    "    main_core_nodes=list(nx.k_core(G).nodes())\n",
    "    ## ahora vemos el core rank de los nodos del main core\n",
    "    main_core_rank=[]\n",
    "    for node in main_core_nodes:\n",
    "        main_core_rank+=[(node,core_rank[node])]\n",
    "    ## ordenamos segun core rank decreciente\n",
    "    main_core_rank=sorted(main_core_rank, key=lambda tup: tup[1],reverse=True)\n",
    "    main_core_rank=list(zip(*main_core_rank))[0]\n",
    "    ## elegimos number_keywords keywords\n",
    "    n=len(main_core_rank)\n",
    "    if n>number_keywords:\n",
    "        return list(main_core_rank[:number_keywords])\n",
    "    else:\n",
    "        return list(main_core_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['genome',\n",
       " 'haplotype',\n",
       " 'archaic',\n",
       " 'human',\n",
       " 'method',\n",
       " 'introgressed',\n",
       " 'allele',\n",
       " 'hominin',\n",
       " 'admixture',\n",
       " 'neanderthal']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_kcore_rank(text_clean,10,8,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) n-gramas de keywords\n",
    "## tomamos las palabras del main core y revisamos el texto original en busqueda de n-gramas de palabras adyacentes de este conjunto de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K es el largo de la ventana\n",
    "# text es el texto original\n",
    "# text_clean es una lista de palabras de un texto ya procesado por clean\n",
    "# dict_words es el diccionario que entrega clean\n",
    "# digraph es el tipo de grafo - True = grafo dirigido, False = grafo no dirigido\n",
    "def main_core_ngrams(text,text_clean,dict_words,K,digraph):\n",
    "    G=graph_weighted(text_clean,K,digraph)\n",
    "    G.remove_edges_from(nx.selfloop_edges(G)) \n",
    "    d=nx.core_number(G) ## diccionario de palabras asociadas a un core\n",
    "    max_core=max(d.values())\n",
    "    text=[w.lower() for w in word_tokenize(text)]\n",
    "    c=[]\n",
    "    for word in text:\n",
    "        if word in dict_words.keys():\n",
    "            if d[dict_words[word]]==max_core:\n",
    "                c+=[word]\n",
    "            else:\n",
    "                c+=['X']\n",
    "        else:\n",
    "            c+=['X']\n",
    "    s=' '.join(c)\n",
    "    ngrams=[token.rstrip().lstrip() for token in s.split('X') if token!=' ' and token!='']\n",
    "    L=[[len(word_tokenize(token)),token] for token in ngrams]\n",
    "    o = OrderedDict()\n",
    "    for x in L:\n",
    "        o.setdefault(x[0], []).append(x[1])\n",
    "    for key in o.keys():\n",
    "        if len(o[key])>1:\n",
    "            o[key]=list(list(zip(*Counter(o[key]).most_common()))[0])\n",
    "        \n",
    "    return dict(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: ['alleles',\n",
       "  'admixture',\n",
       "  'genome',\n",
       "  'eurasia',\n",
       "  'genomes',\n",
       "  'neanderthals',\n",
       "  'methods',\n",
       "  'method',\n",
       "  'category',\n",
       "  'mutational',\n",
       "  'hominin',\n",
       "  'approaches',\n",
       "  's∗',\n",
       "  'humans',\n",
       "  'specific',\n",
       "  'present'],\n",
       " 2: ['introgressed haplotypes',\n",
       "  'modern humans',\n",
       "  'neanderthal genome',\n",
       "  'archaic hominins',\n",
       "  'archaic admixture',\n",
       "  'specific haplotypes',\n",
       "  'archaic hominin',\n",
       "  'archaic haplotypes',\n",
       "  's∗ approach',\n",
       "  'human genomes',\n",
       "  'archaic genomes'],\n",
       " 3: ['modern human genomes',\n",
       "  'modern human haplotypes',\n",
       "  'reference archaic sequence',\n",
       "  'archaic reference genome'],\n",
       " 5: ['reference archaic hominin genome sequences']}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_core_ngrams(text,text_clean,dict_words,8,True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (6) n-gramas de keywords para diferentes valores de k (cores)\n",
    "## Para cada k, extraemos la funcion (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K es el largo de la ventana\n",
    "# text es el texto original\n",
    "# text_clean es una lista de palabras de un texto ya procesado por clean\n",
    "# dict_words es el diccionario que entrega clean\n",
    "# digraph es el tipo de grafo - True = grafo dirigido, False = grafo no dirigido\n",
    "def k_core_ngrams(text,text_clean,dict_words,K,digraph):\n",
    "    G=graph_weighted(text_clean,K,digraph)\n",
    "    G.remove_edges_from(nx.selfloop_edges(G)) \n",
    "    d=nx.core_number(G)\n",
    "    k_core={}\n",
    "    text=[w.lower() for w in word_tokenize(text)]\n",
    "    for core in d.values():\n",
    "        c=[]\n",
    "        for word in text: ## con este for rodeamos las palabras del k core con 'X'\n",
    "            if word in dict_words.keys():\n",
    "                if d[dict_words[word]]==core:\n",
    "                    c+=[word]\n",
    "                else:\n",
    "                    c+=['X']\n",
    "            else:\n",
    "                c+=['X']\n",
    "        s=' '.join(c)\n",
    "        ngrams=[token.rstrip().lstrip() for token in s.split('X') if token!=' ' and token!=''] ## split con 'X'\n",
    "        L=[[len(word_tokenize(token)),token] for token in ngrams]\n",
    "        o = OrderedDict()\n",
    "        for x in L:\n",
    "            o.setdefault(x[0], []).append(x[1])\n",
    "        for key in o.keys():\n",
    "            if len(o[key])>1:\n",
    "                o[key]=list(list(zip(*Counter(o[key]).most_common()))[0]) ## ordenamos los resultados x frecuencia\n",
    "    \n",
    "        k_core[core]=dict(o)\n",
    "        \n",
    "    return k_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{10: {1: ['early']},\n",
       " 11: {1: ['putative',\n",
       "   'rate',\n",
       "   'role',\n",
       "   'genes',\n",
       "   'insights',\n",
       "   'history',\n",
       "   'events',\n",
       "   'impact']},\n",
       " 13: {1: ['data', 'mark', 'study', 's∗-like', 'power', 'suitable'],\n",
       "  2: ['large sets']},\n",
       " 14: {1: ['sankararaman',\n",
       "   'individuals',\n",
       "   'oceania',\n",
       "   'denisovan',\n",
       "   'denisovans',\n",
       "   'unidentified',\n",
       "   'species',\n",
       "   'africa',\n",
       "   'functional',\n",
       "   'phenotypic',\n",
       "   'ancestors',\n",
       "   'information',\n",
       "   'powerful',\n",
       "   'absence',\n",
       "   'unusual',\n",
       "   'characteristics',\n",
       "   'lineage',\n",
       "   'absent',\n",
       "   'rare',\n",
       "   'african',\n",
       "   'recombination',\n",
       "   'distances',\n",
       "   'kb',\n",
       "   'average'],\n",
       "  2: ['suggestive evidence',\n",
       "   'evolutionary consequences',\n",
       "   'recent timing',\n",
       "   'high levels',\n",
       "   'linkage disequilibrium'],\n",
       "  3: ['long divergence time']},\n",
       " 15: {1: ['statistic', 'signature', 'ancient', 'leaves']},\n",
       " 17: {1: ['alleles',\n",
       "   'admixture',\n",
       "   'genome',\n",
       "   'eurasia',\n",
       "   'genomes',\n",
       "   'neanderthals',\n",
       "   'methods',\n",
       "   'method',\n",
       "   'category',\n",
       "   'mutational',\n",
       "   'hominin',\n",
       "   'approaches',\n",
       "   's∗',\n",
       "   'humans',\n",
       "   'specific',\n",
       "   'present'],\n",
       "  2: ['introgressed haplotypes',\n",
       "   'modern humans',\n",
       "   'neanderthal genome',\n",
       "   'archaic hominins',\n",
       "   'archaic admixture',\n",
       "   'specific haplotypes',\n",
       "   'archaic hominin',\n",
       "   'archaic haplotypes',\n",
       "   's∗ approach',\n",
       "   'human genomes',\n",
       "   'archaic genomes'],\n",
       "  3: ['modern human genomes',\n",
       "   'modern human haplotypes',\n",
       "   'reference archaic sequence',\n",
       "   'archaic reference genome'],\n",
       "  5: ['reference archaic hominin genome sequences']}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_core_ngrams(text,text_clean,dict_words,8,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

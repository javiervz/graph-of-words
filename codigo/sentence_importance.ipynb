{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# importancia de oraciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## librerias varias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "import itertools\n",
    "from scipy import spatial\n",
    "import networkx as nx\n",
    "import operator\n",
    "import re\n",
    "from nltk.corpus import stopwords # stopwords de nltk \n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vectores GLOVE\n",
    "### https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = {}\n",
    "f = open('glove.6B.300d.txt') ## 300 dimensiones con 6B palabras (hay versiones con vocabularios mas grandes en el link)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    vectors[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vectores aleatorios para las palabras fuera de vocabulario (se mantienen iguales para el mismo texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_text(text):\n",
    "    text=list(set([w.lower() for w in tokenizer.tokenize(text)]))\n",
    "    text=[w for w in text if w not in stop_words]\n",
    "    text=[w for w in text if not w.isdigit()]\n",
    "    glove_vectors={}\n",
    "    for w in text:\n",
    "        if w not in vectors.keys():\n",
    "            vector=[1]*300\n",
    "            vector=[x*random.uniform(-1.5,1.5) for x in vector]\n",
    "            vector=np.array(vector)\n",
    "            glove_vectors[w]=vector\n",
    "        else:\n",
    "            glove_vectors[w]=vectors[w]\n",
    "    return glove_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## funcion para construir el grafo de oraciones, donde el peso de las aristas indica la similitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simglove(vector_i,vector_j):\n",
    "    return 1-spatial.distance.cosine(vector_i.reshape(1,-1),vector_j.reshape(1,-1))\n",
    "\n",
    "def sent_distance_graph(text): \n",
    "    b = {'et al.': ''} ## saco los 'et al.' pq dan problemas con el sent_tokenize de nltk, debe haber una mejor solucion\n",
    "    for x,y in b.items():\n",
    "        text = text.replace(x, y) \n",
    "    glove_vectors=glove_text(text)\n",
    "    sentences=sent_tokenize(text)\n",
    "    \n",
    "    ### representacion de las oraciones\n",
    "    clean_sentences=[]\n",
    "    for sentence in sentences: \n",
    "        sent=list(set([w.lower() for w in tokenizer.tokenize(sentence)]))\n",
    "        sent=[w for w in sent if w not in stop_words]\n",
    "        sent=[w for w in sent if not w.isdigit()]\n",
    "        clean_sentences+=[sent]\n",
    "    \n",
    "    ### vectores de las oraciones\n",
    "    vector_sentences=[]\n",
    "    for sentence in clean_sentences: \n",
    "        vector_sent=[glove_vectors[word] for word in sentence]\n",
    "        vector_sentences+=[np.mean(vector_sent, axis=0)]\n",
    "    \n",
    "    ### grafo de distancias\n",
    "    n=len(sentences)\n",
    "    pairs=list(itertools.product(range(n),range(n)))\n",
    "    G=nx.Graph()\n",
    "    for node in range(n):\n",
    "        G.add_node(node)\n",
    "    for pair in pairs:\n",
    "        node_i=pair[0]\n",
    "        node_j=pair[1]\n",
    "        if node_i!=node_j:\n",
    "            G.add_edge(node_i,node_j,weight=simglove(vector_sentences[node_i],vector_sentences[node_j]))\n",
    "        else:\n",
    "            G.add_edge(node_i,node_j,weight=0)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## funcion que entrega las K oraciones mas importantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "## esta funcion entrega una lista de oraciones\n",
    "def sentences_pagerank(text,K):\n",
    "    G=sent_distance_graph(text)\n",
    "    b = {'et al.': ''} \n",
    "    for x,y in b.items():\n",
    "        text = text.replace(x, y) \n",
    "    sentences=sent_tokenize(text)\n",
    "    importance_sentences=nx.pagerank(G, alpha=0.85, weight='weight')\n",
    "    ranking=list(list(zip(*sorted(importance_sentences.items(), key=operator.itemgetter(1),reverse=True)))[0])\n",
    "    return [sentences[r] for r in ranking][:K] ## del ranking de oraciones elegimos las K primeras\n",
    "    #return ranking ## si queremos que se recupere el ranking de oraciones (de las mas a las menos importante)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ejemplo - intro sacada de CELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1='Sequencing the Neanderthal genome (Green et al., 2010, Prüfer et al., 2014), the Denisovan genome (Reich et al., 2010), and several early modern human genomes from Eurasia (Fu et al., 2014, Fu et al., 2015) has confirmed that archaic hominins left their mark in the genomes of modern humans (Plagnol and Wall, 2006, Sankararaman et al., 2014, Vernot and Akey, 2014, Vernot et al., 2016). Present-day individuals in Eurasia inherited ∼2% of their genome from Neanderthals (Green et al., 2010), and individuals from Oceania inherited ∼5% of their genome from Denisovans (Reich et al., 2010). Suggestive evidence indicates that admixture from other unidentified hominin species occurred in Africa (Hammer et al., 2011, Hsieh et al., 2016, Lachance et al., 2012, Plagnol and Wall, 2006, Wall et al., 2009).'\n",
    "text2='To understand the functional, phenotypic, and evolutionary consequences of archaic admixture, it is necessary to identify the specific haplotypes and alleles that were inherited from archaic hominin ancestors (Huerta-Sánchez et al., 2014, Juric et al., 2016, Sankararaman et al., 2014, Simonti et al., 2016, Vernot and Akey, 2014). Approaches to identifying introgressed haplotypes include methods that specifically incorporate reference archaic hominin genome sequences and reference-free methods that do not utilize such information. An example of the former category is the method of Sankararaman et al. (2014), which identifies archaic haplotypes by comparing modern human haplotypes to a reference archaic sequence. The latter category of methods include the S∗ statistic (Plagnol and Wall, 2006), which searches for the mutational signature that ancient admixture leaves in the genomes of present-day humans.' \n",
    "text3='The S∗ approach is powerful for finding introgressed haplotypes in the absence of an archaic reference genome because it leverages the unusual mutational characteristics of introgressed haplotypes. Because of the long divergence time between Neanderthals and modern humans, Neanderthals carry many alleles that are specific to their lineage. Such alleles are present on introgressed haplotypes but are absent or rare in African genomes.'\n",
    "text4='Further, based on the recent timing of admixture, introgressed haplotypes are expected to be maintained without recombination over distances of approximately 50 kb on average (Sankararaman et al., 2012), resulting in high levels of linkage disequilibrium (LD) between Neanderthal-specific alleles in non-African human genomes. In this study, we develop an S∗-like method that has increased power and is suitable for large-scale genome-wide data. We apply the method to large sets of sequenced data from Eurasia and Oceania and identify putative archaic-specific alleles. We examine the rate at which these alleles match the sequenced archaic genomes and the role of the genes containing these alleles, to obtain insights into the history of the admixture events and their impact on modern human genomes.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sequencing the Neanderthal genome (Green , 2010, Prüfer , 2014), the Denisovan genome (Reich , 2010), and several early modern human genomes from Eurasia (Fu , 2014, Fu , 2015) has confirmed that archaic hominins left their mark in the genomes of modern humans (Plagnol and Wall, 2006, Sankararaman , 2014, Vernot and Akey, 2014, Vernot , 2016).']\n",
      "['Approaches to identifying introgressed haplotypes include methods that specifically incorporate reference archaic hominin genome sequences and reference-free methods that do not utilize such information.']\n",
      "['The S∗ approach is powerful for finding introgressed haplotypes in the absence of an archaic reference genome because it leverages the unusual mutational characteristics of introgressed haplotypes.']\n",
      "['We examine the rate at which these alleles match the sequenced archaic genomes and the role of the genes containing these alleles, to obtain insights into the history of the admixture events and their impact on modern human genomes.']\n"
     ]
    }
   ],
   "source": [
    "print(sentences_pagerank(text1,1))\n",
    "print(sentences_pagerank(text2,1))\n",
    "print(sentences_pagerank(text3,1))\n",
    "print(sentences_pagerank(text4,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
